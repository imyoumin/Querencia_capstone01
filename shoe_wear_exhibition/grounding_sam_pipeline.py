# grounding_sam_pipeline.py

import numpy as np
import torch
from PIL import Image

from transformers import (
    AutoProcessor,
    AutoModelForZeroShotObjectDetection,
    SamProcessor,
    SamModel,
    infer_device,
)

# =========================
# ê¸°ë³¸ ì„¤ì •
# =========================

DEVICE = infer_device()

# ğŸ‘‰ ì „ì‹œì¥ PCì—ì„œ ì™„ì „ ì˜¤í”„ë¼ì¸ìœ¼ë¡œ ì“°ê³  ì‹¶ìœ¼ë©´
#    ì•„ë˜ ë‘ ì¤„ì„ "models/grounding_dino_tiny", "models/sam_vit_base"
#    ì²˜ëŸ¼ ë¡œì»¬ í´ë” ê²½ë¡œë¡œ ë°”ê¿”ë„ ë¨.
GROUNDING_DINO_ID = "IDEA-Research/grounding-dino-tiny"  # ë˜ëŠ” "models/grounding_dino_tiny"
SAM_ID = "facebook/sam-vit-base"                         # ë˜ëŠ” "models/sam_vit_base"

# ë°°ê²½ íšŒìƒ‰(ì¤‘ê°„ íšŒìƒ‰)
BG_VALUE = 128  # 0=ê²€ì •, 255=í°ìƒ‰, 128=ì¤‘ê°„ íšŒìƒ‰

# ëª¨ë¸ì„ ì „ì—­ìœ¼ë¡œ í•œ ë²ˆë§Œ ë¡œë“œí•˜ê¸° ìœ„í•œ ìºì‹œ
_gd_processor = None
_gd_model = None
_sam_processor = None
_sam_model = None


# =========================
# ëª¨ë¸ ë¡œë“œ í•¨ìˆ˜
# =========================

def _load_models():
    """Grounding DINO / SAM ëª¨ë¸ì„ ì „ì—­ìœ¼ë¡œ í•œ ë²ˆë§Œ ë¡œë“œ."""
    global _gd_processor, _gd_model, _sam_processor, _sam_model

    if _gd_processor is None or _gd_model is None:
        print("[grounding_sam_pipeline] Loading Grounding DINO from:", GROUNDING_DINO_ID)
        _gd_processor = AutoProcessor.from_pretrained(GROUNDING_DINO_ID)
        _gd_model = AutoModelForZeroShotObjectDetection.from_pretrained(
            GROUNDING_DINO_ID
        ).to(DEVICE)

    if _sam_processor is None or _sam_model is None:
        print("[grounding_sam_pipeline] Loading SAM from:", SAM_ID)
        _sam_processor = SamProcessor.from_pretrained(SAM_ID)
        _sam_model = SamModel.from_pretrained(SAM_ID).to(DEVICE)


# =========================
# ë©”ì¸ íŒŒì´í”„ë¼ì¸ í•¨ìˆ˜
# =========================

def run_grounding_sam(image: Image.Image):
    """
    1. Grounding DINOë¡œ 'a pair of shoes' ë°•ìŠ¤ ê²€ì¶œ
    2. ê°€ì¥ score ë†’ì€ ë°•ìŠ¤ 1ê°œë¥¼ ì„ íƒ
    3. í•´ë‹¹ ë°•ìŠ¤ë¥¼ SAMì— ì…ë ¥í•´ ë§ˆìŠ¤í¬ ìƒì„±
    4. ë§ˆìŠ¤í¬ ë°”ê¹¥ì€ ì¤‘ê°„ íšŒìƒ‰(BG_VALUE)ìœ¼ë¡œ ì±„ì›€
    5. ì‹ ë°œ ì˜ì—­ë§Œ crop (resizeëŠ” í•˜ì§€ ì•ŠìŒ)

    ë°˜í™˜ í˜•ì‹ ì˜ˆ:
    {
        "success": True/False,
        "reason": None ë˜ëŠ” ì‹¤íŒ¨ ì´ìœ  ë¬¸ìì—´,
        "crop": PIL.Image (cropëœ RGB ì´ë¯¸ì§€, ë°°ê²½ì€ íšŒìƒ‰),
        "boxes": [[x0, y0, x1, y1], ...],
        "scores": [float, ...],
        "labels": ["a pair of shoes", ...],
    }
    """
    _load_models()

    # í˜¹ì‹œ ëª¨ë¥¼ grayscale ë“±ì„ ëŒ€ë¹„í•´ í•­ìƒ RGBë¡œ ë³€í™˜
    if image.mode != "RGB":
        image = image.convert("RGB")

    np_image = np.array(image)  # (H, W, 3)
    height, width = np_image.shape[:2]

    # =========================
    # 1) Grounding DINOë¡œ ë°•ìŠ¤ ê²€ì¶œ
    # =========================
    text_labels = [["a pair of shoes"]]

    inputs = _gd_processor(
        images=image,
        text=text_labels,
        return_tensors="pt",
    ).to(DEVICE)

    with torch.no_grad():
        outputs = _gd_model(**inputs)

    results = _gd_processor.post_process_grounded_object_detection(
        outputs=outputs,
        input_ids=inputs.input_ids,
        threshold=0.35,
        text_threshold=0.25,
        target_sizes=[(height, width)],  # (H, W)
    )

    result = results[0]
    boxes = result["boxes"]   # (N, 4)
    scores = result["scores"] # (N,)
    labels = result["labels"] # (N,)

    if len(boxes) == 0:
        return {
            "success": False,
            "reason": "no_shoe_detected",
            "crop": None,
            "boxes": [],
            "scores": [],
            "labels": [],
        }

    # ê°€ì¥ scoreê°€ ë†’ì€ ë°•ìŠ¤ ì„ íƒ
    best_idx = int(torch.argmax(scores).item())
    best_box = boxes[best_idx]
    best_score = float(scores[best_idx].item())
    best_label = labels[best_idx]

    # =========================
    # 2) SAM ì…ë ¥ ì¤€ë¹„
    # =========================
    box_list = best_box.tolist()  # [x0, y0, x1, y1]

    sam_inputs = _sam_processor(
        image,
        input_boxes=[[box_list]],  # [[box]] í˜•íƒœ
        return_tensors="pt",
    ).to(DEVICE)

    with torch.no_grad():
        sam_outputs = _sam_model(**sam_inputs)

    masks = _sam_processor.post_process_masks(
        sam_outputs.pred_masks,
        sam_inputs["original_sizes"],
        sam_inputs["reshaped_input_sizes"],
    )[0]  # (num_boxes, 1, H, W) ë˜ëŠ” (num_boxes, H, W) í˜•íƒœ

    # í•˜ë‚˜ì˜ ë°•ìŠ¤ë§Œ ë„£ì—ˆìœ¼ë‹ˆ 0ë²ˆì§¸ ì‚¬ìš©
    masks_box = masks[0]  # ë³´í†µ (1, H, W) ë˜ëŠ” (H, W)

    if masks_box.ndim == 3:
        # (1, H, W) -> (H, W)
        masks_box = masks_box[0]

    # ì—¬ëŸ¬ ë§ˆìŠ¤í¬ ì±„ë„ì´ ìˆì„ ê²½ìš° maxë¡œ í•©ì¹˜ê¸° (ì•ˆì „ìš©)
    if masks_box.ndim == 3:
        mask_2d = masks_box.max(dim=0).values
    else:
        mask_2d = masks_box

    mask_bin = (mask_2d > 0.5).cpu().numpy()  # (H, W), bool

    ys, xs = np.where(mask_bin)
    if len(ys) == 0 or len(xs) == 0:
        return {
            "success": False,
            "reason": "empty_mask",
            "crop": None,
            "boxes": [best_box.tolist()],
            "scores": [best_score],
            "labels": [best_label],
        }

    # =========================
    # 3) íšŒìƒ‰ ë°°ê²½ìœ¼ë¡œ í•©ì„± + crop
    # =========================
    y_min, y_max = ys.min(), ys.max()
    x_min, x_max = xs.min(), xs.max()

    # íšŒìƒ‰ ë°°ê²½ (H, W, 3)
    bg = np.full_like(np_image, fill_value=BG_VALUE, dtype=np.uint8)

    # ë§ˆìŠ¤í¬ë¥¼ 3ì±„ë„ë¡œ í™•ì¥
    mask_3 = mask_bin.astype(np.uint8)[..., None]  # (H, W, 1)

    # foreground(ì‹ ë°œ)ë§Œ ì›ë³¸ ìœ ì§€, ë‚˜ë¨¸ì§€ëŠ” íšŒìƒ‰ìœ¼ë¡œ ì±„ìš°ê¸°
    composited = np.where(mask_3 == 1, np_image, bg)  # (H, W, 3)

    # ì‹ ë°œ ì˜ì—­ë§Œ crop
    crop_rgb = composited[y_min:y_max + 1, x_min:x_max + 1, :]  # (h, w, 3)
    crop_img = Image.fromarray(crop_rgb, mode="RGB")

    # =========================
    # 4) ê²°ê³¼ ë°˜í™˜ (resize ì•ˆ í•¨)
    # =========================
    return {
        "success": True,
        "reason": None,
        "crop": crop_img,                  # ì‹ ë°œë§Œ ë‚¨ê¸´ crop ì´ë¯¸ì§€ (ë°°ê²½ íšŒìƒ‰, ì‚¬ì´ì¦ˆëŠ” ê°€ë³€)
        "boxes": [best_box.tolist()],      # ì„ íƒëœ ë°•ìŠ¤ 1ê°œ
        "scores": [best_score],
        "labels": [best_label],
    }
